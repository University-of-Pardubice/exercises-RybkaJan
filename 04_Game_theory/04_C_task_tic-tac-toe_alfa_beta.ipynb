{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5457acc-093a-4d4c-b828-b3e5d23d99b1",
   "metadata": {},
   "source": [
    "# Tic-tac-toe minmax algorithm with alpha beta pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f52a5e-f1a6-4381-8f90-ccc2cad21c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1291ef",
   "metadata": {},
   "source": [
    "The original code has been extended with the implementation of the minmax algorithm and with alpha beta pruning.\n",
    "\n",
    "The **minmax** method has been changed to now have alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b7b390-57e3-4256-b28d-4d7d3e73a92f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class State:\n",
    "    \"\"\" Capturing the state of the game\n",
    "    gameplan - two-dimensional 3x3 array\n",
    "             - 0 - empty array\n",
    "             - 1 - X\n",
    "             - 2 - O\n",
    "    player - the player who has the turn in the game\n",
    "\n",
    "    current_player - player who is on the turn in the given state when searching the state space\n",
    "    depth - depth of the state space search\n",
    "    max_depth - maximum length of the search\n",
    "    \"\"\"\n",
    "\n",
    "    generated = 0\n",
    "\n",
    "    def __init__(self, gameplan, player, current_player=None, depth=0, max_depth=100):\n",
    "        self.gameplan = gameplan\n",
    "        self.player = player\n",
    "        if current_player is None:\n",
    "            self.current_player = player\n",
    "        else:\n",
    "            self.current_player = current_player\n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        State.generated += 1\n",
    "\n",
    "    def terminal_test(self):\n",
    "        \"\"\" The method tests the current state and returns a value indicating whether the game is finished and, if so, who the winner is\n",
    "\n",
    "            0 - no final status\n",
    "            1 - 1 wins\n",
    "            2 - winner 2\n",
    "        \"\"\"\n",
    "\n",
    "        # end of game rules - horizontal and vertical triplets\n",
    "        for i in range(3):\n",
    "            if np.array_equal(self.gameplan[i], [1, 1, 1]):\n",
    "                return 1\n",
    "            if np.array_equal(self.gameplan[i], [2, 2, 2]):\n",
    "                return 2\n",
    "            if np.array_equal(self.gameplan[:, i], [1, 1, 1]):\n",
    "                return 1\n",
    "            if np.array_equal(self.gameplan[:, i], [2, 2, 2]):\n",
    "                return 2\n",
    "\n",
    "        # end of game rules - diagonals\n",
    "        if np.array_equal(self.gameplan.diagonal(), [1, 1, 1]):\n",
    "            return 1\n",
    "        if np.array_equal(self.gameplan.diagonal(), [2, 2, 2]):\n",
    "            return 2\n",
    "        if np.array_equal(np.fliplr(self.gameplan).diagonal(), [1, 1, 1]):\n",
    "            return 1\n",
    "        if np.array_equal(np.fliplr(self.gameplan).diagonal(), [2, 2, 2]):\n",
    "            return 2\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def utility(self, result):\n",
    "        \"\"\" The method tests the current state and returns a value indicating whether the game is finished and, if so, who the winner is\n",
    "\n",
    "            0 - no final status\n",
    "            1 - 1 wins\n",
    "            2 - winner 2\n",
    "        \"\"\"\n",
    "        if result == 0:\n",
    "            return 0\n",
    "        elif result == self.player:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def possible_actions(self):\n",
    "        \"\"\" The method returns a list of possible actions for the given state\n",
    "            The action is expressed by the coordinates of an empty playing field.\n",
    "        \"\"\"\n",
    "        possible_actions = []\n",
    "        # Finding empty playing fields\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if self.gameplan[i][j] == 0:\n",
    "                    possible_actions.append((i, j))\n",
    "        return possible_actions\n",
    "\n",
    "    def expand(self, select_action):\n",
    "        \"\"\" The method creates a new game state by applying the selected action to the current state\n",
    "\n",
    "            In the new state, the opposing player will have the turn, but the state will be evaluated from the original player's perspective\n",
    "            The depth of the searched state space will also increase\n",
    "        \"\"\"\n",
    "        # coordinate check\n",
    "        if select_action[0] not in range(3):\n",
    "            return None\n",
    "        if select_action[1] not in range(3):\n",
    "            return None\n",
    "\n",
    "        # playing field must be empty\n",
    "        if self.gameplan[select_action[0], select_action[1]] != 0:\n",
    "            return None\n",
    "        \n",
    "        new_array = np.copy(self.gameplan)\n",
    "        new_array[select_action[0], select_action[1]] = self.current_player\n",
    "        return State(new_array, \n",
    "                     self.player, \n",
    "                     self.next_current_player(), \n",
    "                     self.depth + 1, \n",
    "                     max_depth=self.max_depth)\n",
    "\n",
    "    def minmax(self, strategy=\"max\", alfa=float('-inf'), beta= float('inf')):\n",
    "        \"\"\"\"\n",
    "        The method selects the one that matches the strategy from the possible actions for the given game state\n",
    "\n",
    "        stategy - what strategy will be used to select from the possible actions\n",
    "        alpha - algorithm parameter\n",
    "        beta - algorithm parameter\n",
    "        \"\"\"\n",
    "        \n",
    "        # checking the state of the game, for a completed game the rating from the utility method is returned\n",
    "        result = self.terminal_test()\n",
    "        if result != 0:            \n",
    "            return self.utility(result), action       \n",
    "\n",
    "        # initialization of values for each strategy\n",
    "        if strategy == \"max\":\n",
    "            selected_utilization_value = float('-inf')\n",
    "            next_strategy = \"min\"\n",
    "        else:\n",
    "            selected_utilization_value = float('inf')\n",
    "            next_strategy = \"max\"\n",
    "\n",
    "        # finding possible actions\n",
    "        actions = self.possible_actions()\n",
    "\n",
    "        # the selected action is filled with the first action of the possible actions\n",
    "        selected_action = actions[0]\n",
    "\n",
    "        \n",
    "        # finding the optimum action from all possible acttions\n",
    "        for action in actions:\n",
    "\n",
    "            # state expansion\n",
    "            expanded_state = self.expand(action)\n",
    "\n",
    "            # game end check for expanded condition\n",
    "            result = expanded_state.terminal_test()\n",
    "\n",
    "            if result != 0:\n",
    "                # games end, return status and action rating\n",
    "                return expanded_state.utility(result), action\n",
    "            else:\n",
    "                # the game continues\n",
    "                # if there is at least one state in the expanded state, continue to expand the state, otherwise it's a draw\n",
    "                if len(expanded_state.possible_actions()) == 0:\n",
    "                    utilization = 0\n",
    "                else:\n",
    "                    # recursive call\n",
    "                    # !!! todo - add search depth limitation\n",
    "                    utilization, _ = expanded_state.minmax(next_strategy, alfa, beta)\n",
    "\n",
    "                # according to the strategy, the evaluated action is chosen as the selected action\n",
    "                if strategy == \"max\":\n",
    "                \n",
    "                    if utilization > selected_utilization_value: \n",
    "                        selected_utilization_value = utilization\n",
    "                        selected_action = action\n",
    "                        \n",
    "                    # if the returned minmax value is greater than beta, return it\n",
    "                    if selected_utilization_value >= beta:\n",
    "                        return selected_utilization_value, selected_action\n",
    "                    \n",
    "                    # if the returned value is greater than alpha, update alpha\n",
    "                    if selected_utilization_value > alfa:\n",
    "                        alfa = selected_utilization_value\n",
    "                else:\n",
    "                    # strategy min\n",
    "                    if utilization < selected_utilization_value:\n",
    "                        selected_utilization_value = utilization\n",
    "                        selected_action = action\n",
    "                        \n",
    "                    # if the returned minmax value is less than alpha, return it\n",
    "                    if selected_utilization_value <= alfa:\n",
    "                        return selected_utilization_value, selected_action\n",
    "                    \n",
    "                    # if the returned value is less than beta, update the beta\n",
    "                    if selected_utilization_value < beta:\n",
    "                        beta = selected_utilization_value\n",
    "                        \n",
    "        return selected_utilization_value, selected_action\n",
    "\n",
    "    def next_current_player(self):\n",
    "        \"\"\" The method returns the opponent's for state space searching\n",
    "        \"\"\"\n",
    "        return 3 - self.current_player\n",
    "\n",
    "    def next_player(self):\n",
    "        \"\"\" Method returns opponent\n",
    "        \"\"\"\n",
    "        return 3 - self.player"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e10d826",
   "metadata": {},
   "source": [
    "# Game of tic-tac-toe\n",
    "\n",
    "Creating the initial state of the game\n",
    "* Game plan is empty (0)\n",
    "* Game 1 is on the turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b4adca-e8bf-4977-a89a-bd44d5f8e602",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state = State(gameplan=np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]]),\n",
    "              player=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1589681",
   "metadata": {},
   "source": [
    "The cycle of the game remained unchanged. \n",
    "\n",
    "Run the turn and compare the result of the turn, times and numbers of generated states against the original minmax version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b06d9a-a759-450f-922d-59515d8aac81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Check that the game is not over\n",
    "    game_result = state.terminal_test()\n",
    "    if game_result != 0:\n",
    "        print(f\"Winner is {game_result} \")\n",
    "        break\n",
    "\n",
    "    # Checking for drawn\n",
    "    if len(state.possible_actions()) == 0:\n",
    "        print(\"Drawn\")\n",
    "        break\n",
    "\n",
    "    # player's move\n",
    "    print(f\"=====================\\nPlayer {state.player}\")\n",
    "    _, player_action = state.minmax(\"max\")\n",
    "    print(f\"Select action: {player_action}\")\n",
    "    state = state.expand(player_action)\n",
    "    print(state.gameplan)\n",
    "    print(f\"Generated states {State.generated}.\")\n",
    "    State.generated = 0\n",
    "\n",
    "    # switching the game to the other player\n",
    "    state.player = state.next_player()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340d3be-f984-4633-9453-dca5ee8dc29a",
   "metadata": {},
   "source": [
    "# Task\n",
    "Add a constraint to the algorithm to limit the maximum search depth.\n",
    "\n",
    "Again, you need to change the code in place of # **!!! todo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c11d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = State(gameplan=np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]]),\n",
    "              player=1, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b0a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Check that the game is not over\n",
    "    game_result = state.terminal_test()\n",
    "    if game_result != 0:\n",
    "        print(f\"Winner is {game_result} \")\n",
    "        break\n",
    "\n",
    "    # Checking for drawn\n",
    "    if len(state.possible_actions()) == 0:\n",
    "        print(\"Drawn\")\n",
    "        break\n",
    "\n",
    "    # player's move\n",
    "    print(f\"=====================\\nPlayer {state.player}\")\n",
    "    _, player_action = state.minmax(\"max\")\n",
    "    print(f\"Select action: {player_action}\")\n",
    "    state = state.expand(player_action)\n",
    "    print(state.gameplan)\n",
    "    print(f\"Generated states {State.generated}.\")\n",
    "    State.generated = 0\n",
    "\n",
    "    # switching the game to the other player\n",
    "    state.player = state.next_player()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
