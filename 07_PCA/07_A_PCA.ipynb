{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06240b82-5648-4287-a7fb-0e6f7409add2",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "In this exercise, we will use the same dataset as for the linear regression calculation. \n",
    "\n",
    "Recall that based on the correlation matrix, we selected the variables RM, LSTAT and the explanatory variable MEDV in the model.\n",
    "\n",
    "The developed model had an R2 score of approximately 0.65 on the training and test data.\n",
    "\n",
    "Using PCA, we will try to achieve a better result, i.e. to create a model that will predict better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c9d52-45c1-4ed3-9b40-4dcdc0da8592",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data loading and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ccbc04-2eca-4042-ab9a-dacc58043e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37cd943-173a-4228-9f51-adee1fba55e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv (\"..\\\\dataset\\\\HousingData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1375cb75-f3b4-42c4-a23b-c197523db087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b267acd2-8899-45f5-9b78-2907d3b9bdd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b342c48-fdf0-4316-aba7-1909d0ea9f98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f671fe52-9ef6-46dd-8161-852e0940d00a",
   "metadata": {
    "tags": []
   },
   "source": [
    "The scales of the variables are very different from each other, so we will have to rescale our data to improve its quality because we cannot apply PCA or linear regression to these data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd905d4b-0feb-4057-82a6-e617ad33b9ac",
   "metadata": {},
   "source": [
    "## Linear model of all variables without adjustments\n",
    "Create a control linear model with all variables. \n",
    "\n",
    "The goal is to have a sample model to compare with the refinements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a8994e-9f03-4dab-bd4d-9ae57f096868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a02947",
   "metadata": {},
   "source": [
    "We divide the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b84ed4-295e-4784-9a8b-dfe65eb6280a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.array(data.drop('MEDV',axis=1))\n",
    "Y = np.array(data['MEDV'])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453796da",
   "metadata": {},
   "source": [
    "Creating a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023b1e28-c430-4e28-a369-e830f6bd51a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227d5336",
   "metadata": {},
   "source": [
    "Validation of the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59cfa02-1963-48df-a3de-9262ca55a5d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_pred = lr.predict(X_test)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
    "\n",
    "print(f\"R2 score: {r2}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f36db7",
   "metadata": {},
   "source": [
    "Validation of the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc942c9-e61c-42b1-a595-4d819d61fa60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_pred = lr.predict(X_train)\n",
    "r2 = r2_score(Y_train, Y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(Y_train, Y_pred))\n",
    "\n",
    "print(f\"R2 score: {r2}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55d5983-e78b-4f57-94fa-8402bff97eb9",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "Again we perform a correlation analysis and look for linear dependencies between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b29160-68be-4c5c-977e-969f97f3cd66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "sns.heatmap(data.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b566c8-d7b7-4828-88d8-c5e3f4cef3c0",
   "metadata": {},
   "source": [
    "The **correlation matrix** contains Pearson correlation coefficients between all pairs of variables. The values range from -1 to 1.\n",
    "* 1 → strong positive linear correlation\n",
    "* -1 → strong negative linear correlation\n",
    "* 0 → no linear correlation\n",
    "\n",
    "**Multicollinearity** occurs when predictor variables are strongly correlated with each other (e.g. DIS with INUDS, INOX, AGE).\n",
    "\n",
    "Problem:\n",
    "* Estimates of regression coefficients are unstable and sensitive to small changes in the data.\n",
    "* Interpretation of individual coefficients becomes meaningless because it is impossible to isolate the effect of one variable while holding the others fixed if they are correlated with each other.\n",
    "* Increases the standard error of the coefficients, which reduces statistical significance.\n",
    "\n",
    "\n",
    "The last row is important, as it shows us the linear correlation between the explanatory variables and the explained variable MEDV. Our target variable seems to be highly correlated with LSTAT and RM, which makes sense since these two factors are very important for house pricing.  There is also a lot of multicollinearity.\n",
    "\n",
    "The usual interpretation of the regression coefficient is that it provides an estimate of the effect of a unit change in the independent variable on the dependent variable while holding the other variables constant. In the case of multicollinearity, however, we cannot say this. If X1 is strongly correlated with another independent variable, X2, in a given data set, then we have a set of observations for which X1 and X2 have some linear stochastic relationship. Thus, we cannot ensure when the variable X1 changes, X2 remains constant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56660f07-e446-4011-9612-8ef2a573e130",
   "metadata": {},
   "source": [
    "## Variance Inflation Factor (VIF) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9facdee5-8256-4a7b-803a-fe55aca06d67",
   "metadata": {},
   "source": [
    "The **VIF** detects multicollinearity in regression analysis. \n",
    "\n",
    "Its presence can negatively affect the regression results. The VIF estimates how much the variance of the regression coefficient is inflated due to multicollinearity in the model.\n",
    "\n",
    "VIF=1/(1-R^2)\n",
    "\n",
    "Where R^2 is the coefficient of determination. \n",
    "\n",
    "Simply put, it is the proportion of the variance of the independent variable that is explained by the dependent variable. \n",
    "\n",
    "So we run a linear regression using each variable as the target and the others as predictors and calculate R^2 and then calculate VIF for them.\n",
    "\n",
    "If VIF < 4, the variable can be used, otherwise we have to find a way to remove the collinearity of these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f9a949-e585-44d4-9acf-cb8003464e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vifdf = []\n",
    "for i in data.columns:\n",
    "    X = np.array(data.drop(i,axis=1))\n",
    "    y = np.array(data[i])\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X,y)\n",
    "    y_pred = lr.predict(X)\n",
    "    r2 = r2_score(y,y_pred)\n",
    "    vif = 1/(1-r2)\n",
    "    vifdf.append((i,vif))\n",
    "\n",
    "vifdf = pd.DataFrame(vifdf,columns=['Features','Variance Inflation Factor'])\n",
    "vifdf.sort_values(by='Variance Inflation Factor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9062786a-16b9-4402-b653-a339bd6ab752",
   "metadata": {},
   "source": [
    "We see that almost half of the variables have a VIF value greater than or close to 4. TAX and RAD have a VIF almost twice as high as our threshold.\n",
    "\n",
    "Thus, it will be appropriate to resolve multicollinearity. This can be done in several ways:\n",
    "* Remove correlated variables → select only one of the pair of highly correlated variables.\n",
    "* Principal Component Analysis (PCA) → transform the predictors into uncorrelated components.\n",
    "* Regularization (Ridge, Lasso) → suppresses the effect of collinearity and stabilizes the model.\n",
    "\n",
    "We will look at PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68099e97-b247-4429-a663-87aa38fdf04e",
   "metadata": {},
   "source": [
    "## Data standardization\n",
    "The first step is to standardize the data, so that all variables have a mean of around 0. Then their effect on the output variable will be similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fdf71",
   "metadata": {},
   "source": [
    "Let's look at the data before standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a0d3e-1139-43c4-ba5f-59b5e24b1640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos = 1\n",
    "fig = plt.figure(figsize=(8, 12))\n",
    "for i in data.columns:\n",
    "    ax = fig.add_subplot(7,2, pos)\n",
    "    pos = pos + 1\n",
    "    sns.histplot(data[i],ax=ax, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c3f75",
   "metadata": {},
   "source": [
    "Perform z-standardization using the rescale function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e487fe-386f-45e8-b0c2-96a6747159f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rescale(X):\n",
    "    mean = X.mean()\n",
    "    std = X.std()\n",
    "    scaled_X = [(i - mean)/std for i in X]\n",
    "    return pd.Series(scaled_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a56b22",
   "metadata": {},
   "source": [
    "Create a new standardized dataset data_std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adddb84-518f-4c05-9d18-36ab14f5225c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_std = pd.DataFrame(columns=data.columns)\n",
    "for i in data.columns:\n",
    "    data_std[i] = rescale(data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd69d4fd",
   "metadata": {},
   "source": [
    "To check, let's write down the basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e3a31d-4214-4ade-a958-f6238adac2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_std.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a81100",
   "metadata": {},
   "source": [
    "Display the distribution of values with an estimate of the distribution function.\n",
    "\n",
    "The shape of the distribution of the new variables is the same as for the original variables. Only their mean value is now 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a276e15-a947-4072-b258-970eb733ca4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos = 1\n",
    "fig = plt.figure(figsize=(8,12))\n",
    "for i in data_std.columns:\n",
    "    ax = fig.add_subplot(7,2, pos)\n",
    "    pos = pos + 1\n",
    "    sns.histplot(data_std[i],ax=ax, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2bf1e4",
   "metadata": {},
   "source": [
    "Let's look at the correlation of the standardized data. It remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d2078b-5f45-46f8-9846-3e031101679a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "sns.heatmap(data_std.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5581d412-bea2-4d65-97eb-7e149fa034f1",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "### Idea\n",
    "* If we have a lot of correlated variables, there is hidden redundancy in the data.\n",
    "* PCA removes this redundancy by converting the original variables to new, uncorrelated variables = principal components.\n",
    "* These components are linear combinations of the original variables.\n",
    "\n",
    "\n",
    "### How PCA works (intuition)\n",
    "* Finds the direction with the largest variance of the data (1st principal component).\n",
    "* Finds a second direction with the largest variance but orthogonal to the first (2nd principal component).\n",
    "* Continues until all dimensions are exhausted.\n",
    "* Result:\n",
    "    * Principal components are uncorrelated.\n",
    "    * The first components explain most of the variability in the data.\n",
    "\n",
    "### What PCA is used for\n",
    "* Removing multicollinearity → components are orthogonal → no collinearity.\n",
    "* Dimensionality reduction → keep only the first few components that explain e.g. 90-95% of the variability.\n",
    "* Visualization → complex data from many variables can be plotted in 2D/3D space.\n",
    "\n",
    "PCA is sensitive to the scale of the variables. Therefore, standardization is usually done before applying PCA.\n",
    "\n",
    "We will not write PCA by hand, but use its implementation from the library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fcc1eb-21e6-4b7a-9187-c42eb4132c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f1cbc",
   "metadata": {},
   "source": [
    "The number of PCA components will be 13 as well as input parameters.\n",
    "\n",
    "We have to remove the output MEDV from the input to the PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86c16be-5371-43e4-ac19-66ed31ffc22c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=13)\n",
    "X = data_std.drop('MEDV',axis=1)\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc43794",
   "metadata": {},
   "source": [
    "Now we will create a new data with principal components as input variables and MEDV as output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e29617-7e7f-4225-847c-0c2a10840346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_std_pca = pd.DataFrame(X_pca,columns=['PCA1','PCA2','PCA3','PCA4','PCA5','PCA6','PCA7','PCA8','PCA9','PCA10','PCA11','PCA12','PCA13'])\n",
    "data_std_pca['MEDV'] = data_std['MEDV']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45b1506",
   "metadata": {},
   "source": [
    "PCA was intended to reduce multicollinearity. Let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7421a4db-c2f8-4e77-8bc6-7159bf4421cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot(111)\n",
    "sns.heatmap(data_std_pca.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170a675b-a0b1-47d9-bd3c-53a6c9ef1ce8",
   "metadata": {},
   "source": [
    "The correlation matrix shows that the PCA components are not dependent on each other.\n",
    "\n",
    "MEDV is linearly dependent on the first 3 PCA variables. Then the linear dependence decreases greatly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d53644",
   "metadata": {},
   "source": [
    "The distribution functions of the PCA variables are different from the original ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2384bd-f3ce-4596-a33f-1368dd2bdd43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos = 1\n",
    "fig = plt.figure(figsize=(12,16))\n",
    "for i in data_std_pca.columns:\n",
    "    ax = fig.add_subplot(7,2, pos)\n",
    "    pos = pos + 1\n",
    "    sns.histplot(data_std_pca[i],ax=ax, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e44c6a-fe5f-4948-b5af-51144af50c4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear model of all PCA variables \n",
    "We again divide the data into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b5f096-8c15-46bd-ae17-4a5bba981e08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.array(data_std_pca.drop('MEDV',axis=1))\n",
    "Y = np.array(data_std_pca['MEDV'])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8576f42",
   "metadata": {},
   "source": [
    "We create a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d3faba-cd93-43b7-a7dc-a87d852b2fee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3c7e8d",
   "metadata": {},
   "source": [
    "Model validation for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2886e-492b-42b4-97ca-5e9560a6e01b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_pred = lr.predict(X_train)\n",
    "r2 = r2_score(Y_train, Y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(Y_train, Y_pred))\n",
    "\n",
    "print(f\"R2 score: {r2}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b85358",
   "metadata": {},
   "source": [
    "Model validation for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f74fa-1dcb-4275-8517-38b908df8fb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_pred = lr.predict(X_test)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
    "\n",
    "print(f\"R2 score: {r2}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1a1733",
   "metadata": {},
   "source": [
    "The resulting model from the PCA variables is slightly better than the original linear model from the original variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01aa5bf-fdc6-4136-992e-a0444e9274b4",
   "metadata": {},
   "source": [
    "## Linear model of 6 PCA variables\n",
    "PCA can also be used for dimensionality reduction. \n",
    "\n",
    "So we create a model that has only 6 variables instead of 13 input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae09a4-84c4-469c-b63a-e7704ce12e67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train[:,0:6], Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d959354",
   "metadata": {},
   "source": [
    "Model validation on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87ae656-5f17-4ee8-8b48-0c10f892f69e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_pred = lr.predict(X_train[:,0:6])\n",
    "r2 = r2_score(Y_train, Y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(Y_train, Y_pred))\n",
    "\n",
    "print(f\"R2 score: {r2}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0063fcd",
   "metadata": {},
   "source": [
    "Model validation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef2be9-780a-47e7-9c1e-8664c9d35b15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_pred = lr.predict(X_test[:,0:6])\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
    "\n",
    "print(f\"R2 score: {r2}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7799674",
   "metadata": {},
   "source": [
    "As expected, the precision of the reduced model is slightly lower. On the other hand, the model is smaller."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
